import os
from dotenv import load_dotenv
load_dotenv()
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from langchain_groq import ChatGroq

# Load the GROQ API key from environment variables
GROQ_API_KEY = os.getenv("GROQ_API_KEY")

# Initialize the ChatGroq instance
llm = ChatGroq(temperature=0.5, groq_api_key=GROQ_API_KEY, model_name="llama-3.1-8b-instant")

def generate_viva_questions_and_answer(topics,chat_history):
    prompt_question = ChatPromptTemplate.from_messages([
        ("system", """
        You are a chatbot that will provide one popular viva or oral exam question on the topic or topics - {topics}.
        Provide only one single question and nothing else.
        You never produce same question twice.
        If multiple topics are provided, you can provide a question on any of the topics.
        ASK EASY QUESTIONS WHICH ARE SHORT AND CONCIDE.
        The questions should be theorectical majorly and do not ask to write code.
        The question difficulty should be easy.
        """),
        MessagesPlaceholder(variable_name="chat_history"),
        ("human", "Give me a viva question on {topics}")
    ])

    chain_q = prompt_question | llm

    # Generate the first question automatically
    response_q = chain_q.invoke({"topics": topics, "chat_history": chat_history})
    response_q_content = response_q.content
    chat_history.append(AIMessage(response_q.content))

    prompt_answer = ChatPromptTemplate.from_messages([
        ("system", """
        Answer the question - {response_q_content} - that was generated.
        Answer the question in a simple and straightforward manner like you would in a viva or oral exam in a paragraph manner.
        The answer should be concise and to the point.
        The answer to the question should not be more than 4-5 sentences.
        Give the output as Answer: followed by the answer only.
        """),
        MessagesPlaceholder(variable_name="chat_history"),
        ("human", "Give the answer to the question {response_q_content}")
    ])

    chain_a = prompt_answer | llm

    # Generate the answer automatically
    response_a = chain_a.invoke({"response_q_content": response_q_content, "chat_history": chat_history})
    response_a_content = response_a.content
    chat_history.append(AIMessage(response_a.content))

    return response_q_content.strip(), response_a_content.strip()

# def convert_chat_history_to_json(chat_history): #Uncomment only if you are returning chat_history in viva_qna
#     return [{"type": type(msg).__name__, "content": msg.content} for msg in chat_history]

def generate_feedback(response_q_content, response_a_content, user_answer, chat_history):
    prompt_feedback = ChatPromptTemplate.from_messages([
        ("system", """
        Provide feedback on the answer given by the user ie {user_answer}.
        Compare the user's answer that is {user_answer} with the answer generated by the chatbot- {response_a_content} which is on the same question- {response_q_content} in your backend(for your analysis)
        Generate two scrores- literal score(whether the same words were used or not) and semantic score(whether the meaning is the same or not).
         To summarise-
        So your response should include the following things:
           1.Whether the student's answer is correct or not in one line. If not correct then give the correct answer in less than 20 words
           2.Literal score in percentage and semantic score in percentage       
        Answer in paragraph format which is veryy shortt and concise.
        Talk as if you are a teacher and be polite while giving feedback to a student in a viva or oral exam. directly address the student as 'you'.
        AT THE END YOUR ANSWER MUST NOT BE MORE THAN 30 WORDS ANYTIME.
        IT IS OKAY TO NOT INCLUDE EVERYTHING, BUT DONOT EXCEED 30 WORDS-please make sure this criteria is met!
        """),
        MessagesPlaceholder(variable_name="chat_history"),
        ("human", "Provide feedback on the answer given by the user-{user_answer}")
    ])

    chain_f = prompt_feedback | llm

    # Generate the feedback automatically
    response_f = chain_f.invoke({"response_q_content": response_q_content, "response_a_content": response_a_content, "user_answer": user_answer, "chat_history": chat_history})
    response_f_content = response_f.content
    
    return response_f_content.strip()
