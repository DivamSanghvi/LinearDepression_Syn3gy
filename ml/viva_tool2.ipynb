{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage,AIMessage,SystemMessage\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGroq(temperature=0.5, groq_api_key=GROQ_API_KEY, model_name=\"llama-3.1-8b-instant\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_viva_questions(topics):\n",
    "#     chat_history = []\n",
    "\n",
    "#     prompt = ChatPromptTemplate.from_messages([\n",
    "#         (\"system\", \"\"\"\n",
    "#         You are a chatbot that will provide one popular viva or oral exam question on the topic or topics - {topics}.\n",
    "#         Provide only one single question and nothing else.\n",
    "#          If multiple topics are provided, you can provide a question on any of the topics.\n",
    "#          Keep the question simple and straightforward and avoid complex questions.\n",
    "#          The question difficulty should be easy to medium.\n",
    "\n",
    "#         \"\"\"),\n",
    "#         MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "#         (\"human\",\"Give me a viva question on {topics}\")\n",
    "#     ])\n",
    "\n",
    "#     chain = prompt|llm\n",
    "\n",
    "#     response = chain.invoke({\"topics\":topics,\"chat_history\":chat_history})\n",
    "#     chat_history.append(AIMessage(response.content))\n",
    "#     chat_history.append(HumanMessage(\"Now give me another question on the same topic\"))\n",
    "#     return response.content, chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = generate_viva_questions(\"mvc architecture, react manifesto, aglie methodology\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result[1]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Explain the difference between overfitting and underfitting in machine learning models, and provide a scenario where each occurs.\n",
      "Answer: \n",
      "Overfitting and underfitting are two common issues that can occur in machine learning models. Overfitting occurs when a model is too complex and learns the noise in the training data, resulting in poor performance on new, unseen data. For example, consider a model that is trained to predict house prices based on various features such as number of bedrooms, square footage, and location. If the model includes too many features and becomes overly complex, it may overfit the training data and perform poorly on new data. On the other hand, underfitting occurs when a model is too simple and fails to capture the underlying patterns in the data, resulting in poor performance on both training and test data. A scenario where underfitting occurs is when a linear regression model is used to predict a non-linear relationship between two variables.\n",
      "You've provided a clear and concise explanation of overfitting and underfitting in machine learning models, which is great! However, there are a few areas where you could have done better.\n",
      "\n",
      "You correctly explained that overfitting occurs when a model is too complex and learns the noise in the training data, resulting in poor performance on new data, and provided a good example to illustrate this point. Your explanation of underfitting is also accurate, stating that it occurs when a model is too simple and fails to capture the underlying patterns in the data.\n",
      "\n",
      "However, I noticed that your answer could have benefited from a more detailed explanation of the consequences of overfitting and underfitting, such as how to detect and prevent them. Additionally, you could have provided more specific examples to illustrate the scenarios where each occurs.\n",
      "\n",
      "In terms of literal score, I would give you 80% as your answer uses similar words and concepts to the chatbot's answer, but with some minor variations. However, in terms of semantic score, I would give you 90% as your answer conveys the same meaning and ideas as the chatbot's answer, but with some minor differences in presentation.\n",
      "\n",
      "To improve, you could try to provide more detailed explanations and specific examples to support your points, and also consider varying your sentence structure and wording to make your answer more engaging and easier to read. Keep up the good work!\n",
      "Question: What is the main difference between a generative model and a discriminative model in Machine Learning, and provide an example of each?\n",
      "Answer: The main difference between a generative model and a discriminative model in Machine Learning is their objective function. A generative model aims to learn the underlying distribution of the data and generate new, synthetic data that resembles the original data, whereas a discriminative model is trained to predict the probability of a sample belonging to a particular class or category. An example of a generative model is a Variational Autoencoder (VAE), which generates new images by learning the underlying distribution of a dataset of images. On the other hand, a discriminative model is a logistic regression classifier, which predicts the probability of a sample belonging to a particular class based on its features.\n",
      "fesub, I must say that you've made a good effort to explain the difference between generative and discriminative models in Machine Learning. However, I do have some feedback to help you improve.\n",
      "\n",
      "Firstly, your answer is partially correct, but it lacks some key points. You've mentioned that generative models aim to learn the underlying distribution of the data, which is true, but you haven't explained how this is different from discriminative models. You've also mentioned Variational Autoencoder (VAE) as an example of a generative model, which is correct, but you could have provided more context or details about how it works.\n",
      "\n",
      "In terms of literal score, I would give you 60% because you've used some of the same words and phrases as my answer, but not exactly in the same way. For example, you didn't use the phrase \"objective function\" which is a key concept in Machine Learning.\n",
      "\n",
      "In terms of semantic score, I would give you 70% because, while your answer is not entirely accurate, it does convey the general idea of the difference between generative and discriminative models. However, it's not as clear or concise as my answer.\n",
      "\n",
      "To improve, fesub, I would suggest reading more about generative and discriminative models, and making sure you understand the key concepts and differences between them. You could also try to provide more specific examples and details to support your answer. Keep up the good work, and don't be afraid to ask for help if you're unsure about something!\n",
      "Thank you for using the chatbot!\n"
     ]
    }
   ],
   "source": [
    "# Entire code for generating viva questions, answers and feedback\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "# Load the GROQ API key from environment variables\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "# Initialize the ChatGroq instance\n",
    "llm = ChatGroq(temperature=0.5, groq_api_key=GROQ_API_KEY, model_name=\"llama-3.1-8b-instant\")\n",
    "\n",
    "def generate_viva_questions(topics):\n",
    "    chat_history = []\n",
    "\n",
    "    prompt_question = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"\n",
    "        You are a chatbot that will provide one popular viva or oral exam question on the topic or topics - {topics}.\n",
    "        Provide only one single question and nothing else.\n",
    "        You never produce same question twice.\n",
    "        If multiple topics are provided, you can provide a question on any of the topics.\n",
    "        Keep the question simple and straightforward and avoid complex questions.\n",
    "        The questions should be theorectical majorly and do not ask to write code.\n",
    "        The question difficulty should be easy to medium.\n",
    "        \"\"\"),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"Give me a viva question on {topics}\")\n",
    "    ])\n",
    "\n",
    "    chain_q = prompt_question | llm\n",
    "\n",
    "    # Generate the first question automatically\n",
    "    response_q = chain_q.invoke({\"topics\": topics, \"chat_history\": chat_history})\n",
    "    response_q_content = response_q.content\n",
    "    chat_history.append(AIMessage(response_q.content))\n",
    "    print(f\"Question: {response_q.content.strip()}\")\n",
    "    generate_viva_answer(response_q, chat_history)\n",
    "\n",
    "def generate_viva_answer(response_q_content, chat_history):\n",
    "    prompt_answer = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"\n",
    "        Answer the question - {response_q_content} - that was generated.\n",
    "        Answer the question in a simple and straightforward manner like you would in a viva or oral exam in a paragraph manner.\n",
    "        The answer should be concise and to the point.\n",
    "        The answer to the question should not be more than 4-5 sentences.\n",
    "        Give the output as Answer: followed by the answer only.\n",
    "        \"\"\"),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"Give the answer to the question {response_q_content}\")\n",
    "    ])\n",
    "\n",
    "    chain_a = prompt_answer | llm\n",
    "\n",
    "    # Generate the first question automatically\n",
    "    response_a = chain_a.invoke({\"response_q_content\": response_q_content, \"chat_history\": chat_history})\n",
    "    response_a_content = response_a.content\n",
    "    chat_history.append(AIMessage(response_a.content))\n",
    "    print(f\"{response_a.content.strip()}\")\n",
    "    user_answer = input(\"Enter your answer: \")\n",
    "    generate_feedback(response_q_content,response_a_content, user_answer, chat_history)\n",
    "\n",
    "def generate_feedback(response_q_content, response_a_content,user_answer, chat_history):\n",
    "    prompt_feedback = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"\n",
    "        Provide feedback on the answer given by the user ie {user_answer}.\n",
    "        Compare the user's answer that is {user_answer} with the answer generated by the chatbot- {response_a_content} which is on the same question- {response_q_content}.\n",
    "        See whether the users answer is correct or not by comparing it with the chatbot's answer - the meaning should be similar.\n",
    "        Generate two scrores- literal score(whether the same words were used or not) and semantic score(whether the meaning is the same or not).\n",
    "        Both these scores should be in percentage.\n",
    "        Present your answer in a paragraph manner.\n",
    "        So your answer should include the following things:\n",
    "           1.Whether the student's answer is correct or not. If not correct then give the correct answer\n",
    "           2.Literal score in percentage\n",
    "           3.Semantic score in percentage\n",
    "           4. Any additional comments on what the student could have done better\n",
    "         \n",
    "        These 4 points should be presented in a paragraph manner but keep it short and concise.\n",
    "        Talk as if you are a teacher and be polite while giving feedback to a student in a viva or oral exam. directly address the student as 'you'.\n",
    "        \"\"\"),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"Provide feedback on the answer given by the user-{user_answer}\")\n",
    "    ])\n",
    "\n",
    "    chain_f = prompt_feedback | llm\n",
    "\n",
    "    # Generate the first question automatically\n",
    "    response_f = chain_f.invoke({\"response_q_content\": response_q_content, \"response_a_content\": response_a_content, \"user_answer\": user_answer, \"chat_history\": chat_history})\n",
    "    response_f_content = response_f.content\n",
    "    chat_history.append(AIMessage(response_f.content))\n",
    "    print(f\"{response_f.content.strip()}\")\n",
    "\n",
    "    # Ask the user if they want to continue\n",
    "    user_continue = input(\"Do you want to continue?. Type 'no' if you want to stop: \")\n",
    "    if user_continue.lower() == \"no\":\n",
    "        print(\"Thank you for using the chatbot!\")\n",
    "    else:\n",
    "        generate_viva_questions(\"Machine Learning, Artificial Intelligence\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "topics = \"machine learning, artificial intelligence\"\n",
    "generate_viva_questions(topics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "    You are talking to a 5 year old and make crying sounds everynow and then.\n",
    "    \"\"\"\n",
    "    # Chat history is initialized every time the app or this function is invoked\n",
    "chat_history = [{\"role\": \"system\", \"content\": prompt}]\n",
    "\n",
    "\n",
    "def get_llm_response(question):\n",
    "        # Add the user's question to the chat history\n",
    "        chat_history.append({\"role\": \"user\", \"content\": question})\n",
    "\n",
    "        # Invoke the language model (replace with your actual LLM invocation logic)\n",
    "        llm = ChatGroq(temperature=0, groq_api_key=GROQ_API_KEY, model_name=\"llama-3.1-8b-instant\")\n",
    "        response = llm.invoke(chat_history)\n",
    "\n",
    "        # Append the assistant's response to the chat history\n",
    "        chat_history.append({\"role\": \"assistant\", \"content\": response.content})\n",
    "\n",
    "        return response.content\n",
    "\n",
    "    return get_llm_response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
